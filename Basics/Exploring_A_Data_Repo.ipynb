{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring a Data Repository\n",
    "\n",
    "<br>Owner: **Phil Marshall** ([@drphilmarshall](https://github.com/LSSTScienceCollaborations/StackClub/issues/new?body=@drphilmarshall))\n",
    "<br>Last Verified to Run: **2018-09-07**\n",
    "<br>Verified Stack Release: **16.0**\n",
    "\n",
    "This notebook shows how to find out what's in a data repository, and how to find out which inputs went into each component of it.  \n",
    "\n",
    "### Learning Objectives:\n",
    "After working through and studying this notebook you should be able to understand how to use the Butler to figure out: \n",
    "   1. Which data types are present in a data repository\n",
    "   2. If coadds have been made, what the available tracts and patches are\n",
    "   3. TBD\n",
    "   \n",
    "### Logistics\n",
    "This notebook is intended to be runnable on `lsst-lspdev.ncsa.illinois.edu` from a local git clone of https://github.com/LSSTScienceCollaborations/StackClub.\n",
    "\n",
    "\n",
    "## Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Filter some warnings printed by v16.0 of the stack\n",
    "warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "warnings.simplefilter(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The HSC Dataset: What's in there?\n",
    "We'll use the `hsc` dataset as our testing ground, and start by figuring out what's there.\n",
    "\n",
    "We'll need a butler to interrogate the `hsc` data repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lsst.daf.persistence import Butler\n",
    "\n",
    "# Instantiate the butler to bring us some HSC data.\n",
    "\n",
    "depth = 'WIDE' # WIDE, DEEP, UDEEP\n",
    "field = 'SSP_WIDE' # SSP_WIDE, SSP_DEEP, SSP_UDEEP\n",
    "\n",
    "repo = '/datasets/hsc/repo/rerun/DM-13666/%s/'%(depth)\n",
    "butler = Butler(repo)\n",
    "\n",
    "print(repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasetRefOrType : forced_src\n",
    "# see all options at\n",
    "# /opt/lsst/software/stack/stack/miniconda3-4.3.21-10a4fa6/Linux64/obs_subaru/16.0+1/python/lsst/obs/hsc\n",
    "\n",
    "# Better to point to a resource online:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stackclub import where_is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where_is('ingest.py', in_the='source', assuming_its_a='cmdlinetask')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `HscMapper` class is defined in [HscMapper.py](https://github.com/lsst/obs_subaru/blob/master/python/lsst/obs/hsc/hscMapper.py). Let's read about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lsst.obs.hsc import HscMapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(HscMapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mapper defines a (large) number of different dataset types. Some of these are specific to this particular dataset, others are more general. Even filtering out some intermediate dataset types, we are still left with a long list..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = HscMapper(root=repo)\n",
    "all_dataset_types = mapper.getDatasetTypes()\n",
    "\n",
    "remove = ['_config', '_filename', '_md', '_sub', '_len', '_schema', '_metadata']\n",
    "\n",
    "shortlist = []\n",
    "for dataset_type in all_dataset_types:\n",
    "    keep = True\n",
    "    for word in remove:\n",
    "        if word in dataset_type:\n",
    "            keep = False\n",
    "    if keep:\n",
    "        shortlist.append(dataset_type)\n",
    "\n",
    "print(shortlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `butler` purpoorts to be able to check whether a dataset actually exists or not, but needs a specific dataset ID to check it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "butler.datasetExists('calexp', dataId={})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, one can try querying the metadata and checking for an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasettype = 'calexp'\n",
    "\n",
    "try:\n",
    "    datasetkeys = butler.getKeys(datasettype)\n",
    "    onekey = list(datasetkeys.keys())[0]\n",
    "    metadata = butler.queryMetadata(datasettype, [onekey])\n",
    "    print(\"{} dataset exists.\".format(datasettype))\n",
    "except:\n",
    "    print(\"{} dataset doesn't exist.\".format(datasettype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(butler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return basic dataset properties\n",
    "For this dataset, we can look at the filters used, number of visits, number of pointings, etc. by examining the Butler's keys and metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interesting dataset types for the HSC_mapper dataset. \n",
    "datasettypes = ['calexp', 'calexpBackground', 'icSrc', \n",
    "                'src', 'srcMatch', 'srcMatchFull', 'ossImage', \n",
    "                'flattenedImage', 'wcs', 'fcr', 'photoCalib',\n",
    "                'jointcal_wcs', 'jointcal_photoCalib', 'skyCorr',\n",
    "                'calexp_camera', 'brightObjectMask', 'deepCoadd_calexp', \n",
    "                'deepCoadd_det', 'deepCoadd_meas', 'deepCoadd_measMatch', \n",
    "                'deepCoadd_mergeDet', 'deepCoadd_ref', 'deepCoadd_forced_src', \n",
    "                'forced_src' ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these basic properties, we will look at the `calexp` and `src` tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This would be faster if only one query were issued\n",
    "visits = butler.queryMetadata('calexp', ['visit'])\n",
    "pointings = butler.queryMetadata('calexp', ['pointing'])\n",
    "ccds = butler.queryMetadata('calexp', ['ccd'])\n",
    "fields = butler.queryMetadata('calexp', ['field'])\n",
    "filters = butler.queryMetadata('calexp', ['filter'])\n",
    "\n",
    "# Collect number of objects from Source Catalog\n",
    "sources = butler.queryMetadata('src', ['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_visits = len(visits)\n",
    "num_pointings = len(pointings)\n",
    "num_ccds = len(ccds)\n",
    "num_fields = len(fields)\n",
    "num_filters = len(filters)\n",
    "\n",
    "num_sources = len(sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One key quantity for astronomers is the total sky area imaged. We can estimate this from the coadd tract info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect tracts from files\n",
    "import os, glob\n",
    "tracts = sorted([int(os.path.basename(x)) for x in\n",
    "                 glob.glob(os.path.join(repo, 'deepCoadd-results', 'merged', '*'))])\n",
    "num_tracts = len(tracts)\n",
    "\n",
    "#Note: I'd like to do this with the butler, but it appears 'tracts' have to be\n",
    "#      specified in the dataId to be queried, so the queryMetadata method fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate area from all tracts\n",
    "skyMap = butler.get('deepCoadd_skyMap')\n",
    "total_area = 0.0  #deg^2\n",
    "for test_tract in tracts:\n",
    "    # Get inner vertices for tract\n",
    "    tractInfo = skyMap[test_tract]\n",
    "    vertices = tractInfo._vertexCoordList\n",
    "    \n",
    "    #calculate area of box\n",
    "    av_dec = 0.5 * (vertices[2][1] + vertices[0][1])\n",
    "    av_dec = av_dec.asRadians()\n",
    "    delta_ra_raw = vertices[0][0] - vertices[1][0] \n",
    "    delta_ra = delta_ra_raw.asDegrees() * np.cos(av_dec)\n",
    "    delta_dec= vertices[2][1] - vertices[0][1]\n",
    "    area = delta_ra * delta_dec.asDegrees()\n",
    "    \n",
    "    #combine areas\n",
    "    total_area += area\n",
    "    \n",
    "print(\"Total area imaged (sq deg): \",total_area)\n",
    "\n",
    "#round total area for table purposes\n",
    "rounded_total_area = round(total_area, 2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out a report of the metadata\n",
    "\n",
    "# dataset_name = 'HSC'\n",
    "# display(Markdown('# Dataset: %s' %dataset_name))\n",
    "\n",
    "# A more automated version of the table title:\n",
    "dataset_name = 'HSC'\n",
    "display(Markdown('# %s' % repo))\n",
    "\n",
    "\n",
    "# Make a table of the collected metadata\n",
    "collected_data = [num_visits, num_pointings, num_ccds, num_fields, num_filters, num_sources, \n",
    "                  num_tracts, rounded_total_area]\n",
    "data_names = (\"Number of Visits\", \"Number of Pointings\", \"Number of CCDs\", \"Number of Fields\", \n",
    "              \"Number of Filters\", \"Number of Sources\", \"Number of Tracts\", \"Total Sky Area (deg$^2$)\")\n",
    "\n",
    "output_table = \"|   Metadata Characteristics  |  | \\n  | :---: | --- | \\n \"\n",
    "counter = 0\n",
    "while counter < len(collected_data):\n",
    "    output_table += \"| %s |  %s | \\n\" %(data_names[counter], collected_data[counter])\n",
    "    counter += 1\n",
    "display(Markdown(output_table))\n",
    "\n",
    "# Show which fields and filters we're talking about:\n",
    "display(Markdown('Fields: (%i total)' %num_fields))\n",
    "print(fields)\n",
    "display(Markdown('Filters: (%i total)' %num_filters))\n",
    "print(filters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sandbox: Initial Exploration of Tracts and Patches, Coadd Images\n",
    "\n",
    "_To be deleted eventually..._\n",
    "\n",
    "Let's try exploring the `hsc` dataset's coadd images, and the visits that went into them. Jim Bosch shows how to do this in [this community.lsst.org post](https://community.lsst.org/t/visualizing-source-images-in-a-coadd/441/2).\n",
    "\n",
    "We'll need a single coadd image to work from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lsst.daf.persistence import Butler\n",
    "\n",
    "# Instantiate the butler\n",
    "depth = 'WIDE' # WIDE, DEEP, UDEEP\n",
    "field = 'SSP_WIDE' # SSP_WIDE, SSP_DEEP, SSP_UDEEP\n",
    "repo = '/datasets/hsc/repo/rerun/DM-13666/%s/'%(depth)\n",
    "butler = Butler(repo)\n",
    "\n",
    "# The following does not work, because ci_hsc has not been ingested or reduced!\n",
    "# butler = Butler('/project/shared/data/ci_hsc/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jim's function for returning the visits that went into a coadd:\n",
    "def showInputs(butler, dataId, coaddType=\"deepCoadd\"):\n",
    "    coadd = butler.get(coaddType, **dataId)\n",
    "    visitInputs = coadd.getInfo().getCoaddInputs().visits\n",
    "    ccdInputs = coadd.getInfo().getCoaddInputs().ccds\n",
    "    ccdDict = dict((int(v), int(ccd)) for v, ccd in zip(ccdInputs.get(\"visit\"), ccdInputs.get(\"ccd\")))\n",
    "    for v in visitInputs.get(\"id\"):\n",
    "        md = butler.get(\"calexp_md\", visit=int(v), ccd=ccdDict[v])\n",
    "        print(\"%d %4.0f\" % (v, afwImage.Calib(md).getExptime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all tracts, and choose the first one we come to:\n",
    "import os, glob\n",
    "tracts = sorted([int(os.path.basename(x)) for x in\n",
    "                 glob.glob(os.path.join(repo, 'deepCoadd-results', 'merged', '*'))])\n",
    "tract = tracts[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all patches in our tract, and choose the first one we come to. For this, we need the skymap:\n",
    "skyMap = butler.get('deepCoadd_skyMap')\n",
    "tractInfo = skyMap[tract]\n",
    "patchInfo = tractInfo.getPatchInfo([0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tractInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patchInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain a random dataid for a coadd image in the i-band\n",
    "band = 'HSC-I'\n",
    "patch = (0,0)\n",
    "\n",
    "subset = butler.subset('deepCoadd', dataId={'filter':band, 'tract':tract, 'patch':patch})\n",
    "dataid = subset.cache[0]\n",
    "print(dataid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coadd = butler.get('deepCoadd', **dataid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showInputs(butler, dataid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSST",
   "language": "python",
   "name": "lsst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
