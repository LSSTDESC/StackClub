{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring a Data Repository\n",
    "\n",
    "<br>Owner: **Rob Morgan** ([@rmorgan10](https://github.com/LSSTScienceCollaborations/StackClub/issues/new?body=@rmorgan10)), **Phil Marshall** ([@drphilmarshall](https://github.com/LSSTScienceCollaborations/StackClub/issues/new?body=@drphilmarshall))\n",
    "<br>Last Verified to Run: **2018-12-07**\n",
    "<br>Verified Stack Release: **17.0**\n",
    "\n",
    "This notebook shows how to find out what's in a data repository, and how to find out which inputs went into each component of it.  \n",
    "\n",
    "### Learning Objectives:\n",
    "After working through and studying this notebook you should be able to understand how to use the Butler to figure out: \n",
    "   1. What a data repo is;\n",
    "   2. Which data types are present in a data repository;\n",
    "   3. If coadds have been made, what the available tracts are;\n",
    "   4. Which parts of the sky those tracts cover.\n",
    "   \n",
    "### Logistics\n",
    "This notebook is intended to be runnable on `lsst-lsp-stable.ncsa.illinois.edu` from a local git clone of https://github.com/LSSTScienceCollaborations/StackClub.\n",
    "\n",
    "\n",
    "## Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Markdown\n",
    "import numpy as np\n",
    "import os, glob\n",
    "%matplotlib inline\n",
    "\n",
    "# Filter some warnings printed by v16.0 of the stack\n",
    "# warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "# warnings.simplefilter(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Data Repo?\n",
    "\n",
    "Data repositories contain either a `_mapper` file or a `repositoryCfg.yaml` file, to record which \"obs package\" was used to organize the data. These files give a repository more strucutre and organization than an ordinary data directory. Let's take a look at this file structure in the HSC data repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The HSC Data Repo: What's in there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = '/datasets/hsc/repo'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the `hsc` data repository as our testing ground, and start by figuring out what it contains. In the `hsc` case, the `_mapper` file is in the top level folder, while the data repo for each field is a few levels down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls /datasets/hsc/repo/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the `_mapper` file here, and at contains one line giving the name of the `Mapper` object for the HSC repo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat /datasets/hsc/repo/_mapper\n",
    "\n",
    "# Import the Mapper object once you know its name\n",
    "from lsst.obs.hsc import HscMapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get some more information on this object like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(HscMapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mapper defines a (large) number of different \"dataset types\". Some of these are specific to this particular data repo, others are more general. Even filtering out some intermediate dataset types, we are still left with a long list. But, once we figure out which dataset types we are interested in, we can start querying for information about those datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = HscMapper(root=repo)\n",
    "all_dataset_types = mapper.getDatasetTypes()\n",
    "\n",
    "remove = ['_config', '_filename', '_md', '_sub', '_len', '_schema', '_metadata']\n",
    "\n",
    "shortlist = []\n",
    "for dataset_type in all_dataset_types:\n",
    "    keep = True\n",
    "    for word in remove:\n",
    "        if word in dataset_type:\n",
    "            keep = False\n",
    "    if keep:\n",
    "        shortlist.append(dataset_type)\n",
    "\n",
    "print(shortlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Butler`, directed by the `Mapper` will have access to all the above dataset types. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Great, but where's the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next level to dig into here is the `rerun` directory. This is where we will find some of the data we can take a look at. In the next few cells, we'll see how to get from the main level of the directory to where the data and `repositoryCfg.yaml` file is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls /datasets/hsc/repo/rerun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls /datasets/hsc/repo/rerun/DM-13666"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls /datasets/hsc/repo/rerun/DM-13666/WIDE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats! Now we're getting to some data! The numerical directories are the tract indices and digging deeper into those directories is a starting point for familiarizing oneself with the repo. We'll skip that here and move on to looking at the data with the `Butler`.\n",
    "\n",
    "The `Butler` will look at the repository as a whole. To see why this is, take a look at `repositoryCfg.yaml` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat /datasets/hsc/repo/rerun/DM-13666/WIDE/repositoryCfg.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `_root` parameter is directing the `Butler` to look at the top level of the repo `/datasets/hsc/repo/`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiating the Butler and looking for Dataset Types\n",
    "\n",
    "Now that we have an idea of the structure of the repo itself, let's use the Butler to explore the data within the repo. Here we will demonstrate a few useful `Butler` methods for learning about the data in a repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lsst.daf.persistence import Butler\n",
    "\n",
    "butler = Butler(repo)\n",
    "\n",
    "# We defined 'repo' earlier, but here it is:\n",
    "print(repo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `butler` purports to be able to check whether a dataset actually exists or not, but it needs a specific dataset ID to check whether that specific dataset exists. Here's what you get when you pass in a null dataset ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "butler.datasetExists('calexp', dataId={})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, one can try querying the metadata and checking for an error. Note that this way of checking for dataset existence is a little faster too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasettype = 'calexp'\n",
    "\n",
    "try:\n",
    "    datasetkeys = butler.getKeys(datasettype)\n",
    "    onekey = list(datasetkeys.keys())[0]\n",
    "    metadata = butler.queryMetadata(datasettype, [onekey])\n",
    "    print(\"{} dataset exists.\".format(datasettype))\n",
    "except:\n",
    "    print(\"{} dataset doesn't exist.\".format(datasettype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining Basic Dataset Properties Using the Butler\n",
    "Now we can start using Butler methods to query the data. For this dataset, we can look at the filters used, number of visits, number of pointings, etc. by examining the Butler's keys and metadata. For these basic properties, we will look at the `calexp` and `src` tables. The contents of these tables are derived from the processing of individual sensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This would be faster if only one query were issued\n",
    "visits = butler.queryMetadata('calexp', ['visit'])\n",
    "pointings = butler.queryMetadata('calexp', ['pointing'])\n",
    "ccds = butler.queryMetadata('calexp', ['ccd'])\n",
    "fields = butler.queryMetadata('calexp', ['field'])\n",
    "filters = butler.queryMetadata('calexp', ['filter'])\n",
    "\n",
    "# Collect number of objects from Source Catalog\n",
    "sources = butler.queryMetadata('src', ['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_visits = len(visits)\n",
    "num_pointings = len(pointings)\n",
    "num_ccds = len(ccds)\n",
    "num_fields = len(fields)\n",
    "num_filters = len(filters)\n",
    "\n",
    "num_sources = len(sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One may also be interested in the total sky area imaged for a particular coadd rerun/depth. We can estimate and visualize this from the coadd tract info. To collect all the tracts, we have to get them via the file strucutre. This operation will hopefully be `Butler`-ized with the Gen3 Butler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the loaction of the tracts\n",
    "rerun = 'DM-13666'\n",
    "depth = 'WIDE'\n",
    "# Try a different one:\n",
    "# rerun = 'DM-10404'\n",
    "# depth = 'DEEP'\n",
    "\n",
    "tract_location = repo + '/rerun/' + rerun + '/' + depth\n",
    "\n",
    "# Collect tracts from files\n",
    "tracts = sorted([int(os.path.basename(x)) for x in\n",
    "                 glob.glob(os.path.join(tract_location, 'deepCoadd-results', 'merged', '*'))])\n",
    "num_tracts = len(tracts)\n",
    "\n",
    "print(\"Found {} tracts in the directory {}\".format(num_tracts, tract_location))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick way of extimating the sky area covered is to sum the areas of the inner boxes of all the tracts. For more information on the properties of tracts, you can look at the [Documentation](http://doxygen.lsst.codes/stack/doxygen/x_masterDoxyDoc/classlsst_1_1skymap_1_1tract_info_1_1_tract_info.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a quick note, the file structure only tells us the names of the tracts in the particular rerun/depth to look at. The actual `TractInfo` objects are obtained by selecting the tracts we want from the `deepCoadd_skyMap` dataset. Therefore, we will have to ask the `Butler` to bring us this dataset for the particular rerun/depth. Since we already made a Butler for the repo as a whole, let's make a `skymap_butler` to bring us the skymap correspoinding to the `tract_location` we just specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "under_butler = Butler(tract_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate area from all tracts\n",
    "skyMap = under_butler.get('deepCoadd_skyMap')\n",
    "total_area = 0.0  #deg^2\n",
    "plotting_vertices = []\n",
    "for test_tract in tracts:\n",
    "    # Get inner vertices for tract\n",
    "    tractInfo = skyMap[test_tract]\n",
    "    vertices = tractInfo._vertexCoordList\n",
    "    plotting_vertices.append(vertices)\n",
    "    \n",
    "    #calculate area of box\n",
    "    av_dec = 0.5 * (vertices[2][1] + vertices[0][1])\n",
    "    av_dec = av_dec.asRadians()\n",
    "    delta_ra_raw = vertices[0][0] - vertices[1][0] \n",
    "    delta_ra = delta_ra_raw.asDegrees() * np.cos(av_dec)\n",
    "    delta_dec= vertices[2][1] - vertices[0][1]\n",
    "    area = delta_ra * delta_dec.asDegrees()\n",
    "    \n",
    "    #combine areas\n",
    "    total_area += area\n",
    "    \n",
    "print(\"Total area imaged (sq deg): \",total_area)\n",
    "\n",
    "#round total area for table purposes\n",
    "rounded_total_area = round(total_area, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also pull out, for this rerun, the same basic information that we did for the whole repo, and compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rerun_visits = under_butler.queryMetadata('calexp', ['visit'])\n",
    "rerun_pointings = under_butler.queryMetadata('calexp', ['pointing'])\n",
    "rerun_ccds = under_butler.queryMetadata('calexp', ['ccd'])\n",
    "rerun_fields = under_butler.queryMetadata('calexp', ['field'])\n",
    "rerun_filters = under_butler.queryMetadata('calexp', ['filter'])\n",
    "\n",
    "rerun_sources = under_butler.queryMetadata('src', ['id'])\n",
    "\n",
    "rerun_num_visits = len(rerun_visits)\n",
    "rerun_num_pointings = len(rerun_pointings)\n",
    "rerun_num_ccds = len(rerun_ccds)\n",
    "rerun_num_fields = len(rerun_fields)\n",
    "rerun_num_filters = len(rerun_filters)\n",
    "\n",
    "rerun_num_sources = len(rerun_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The HSC repo contains {} visits, the {}/{} rerun contains {} visits.\".format(num_visits,rerun,depth,rerun_num_visits))\n",
    "print(\"The HSC repo contains {} pointings, the {}/{} rerun contains {} pointings.\".format(num_pointings,rerun,depth,rerun_num_pointings))\n",
    "print(\"The HSC repo contains {} ccds, the {}/{} rerun contains {} ccds.\".format(num_ccds,rerun,depth,rerun_num_ccds))\n",
    "print(\"The HSC repo contains {} fields, the {}/{} rerun contains {} fields.\".format(num_fields,rerun,depth,rerun_num_fields))\n",
    "print(\"The HSC repo contains {} filters, the {}/{} rerun contains {} filters.\".format(num_filters,rerun,depth,rerun_num_filters))\n",
    "print(\"The HSC repo contains {} sources, the {}/{} rerun contains {} sources.\".format(num_sources,rerun,depth,rerun_num_sources))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes sense, given what we saw in the `repositoryCfg.yaml` file in the DM-13666/WIDE subfolder. What is less clear is why it is like this - one would think that each rerun would lead to a slightly different number of sources, at least - and when the main `butler` queries for sources, where is it getting the information from? Did we just get lucky somehow, choosing DM-13666/WIDE? \n",
    "\n",
    "When I choose a different rerun and depth, I get the exact same numbers, except for the sky area: DM-10404/DEEP only covers 103.8 sq deg. So this number of sources is confusing: where is it coming from? Why does the metadata queried by a butler attached to DM-10404/DEEP give the same number of sources as that from DM-13666/WIDE?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying Dataset Characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's print out a report of all the characteristcs we have found. We'll use the sky area from the DM-13666/WIDE rerun and the numbers common to all reruns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'HSC'\n",
    "display(Markdown('### %s' % repo))\n",
    "\n",
    "\n",
    "# Make a table of the collected metadata\n",
    "collected_data = [num_visits, num_pointings, num_ccds, num_fields, num_filters, num_sources, \n",
    "                  num_tracts, rounded_total_area]\n",
    "data_names = (\"Number of Visits\", \"Number of Pointings\", \"Number of CCDs\", \"Number of Fields\", \n",
    "              \"Number of Filters\", \"Number of Sources\", \"Number of Tracts\", \"Total Sky Area (deg$^2$)\")\n",
    "\n",
    "output_table = \"|   Metadata Characteristic  | Value | \\n  | ---: | ---: | \\n \"\n",
    "counter = 0\n",
    "while counter < len(collected_data):\n",
    "    output_table += \"| %s |  %s | \\n\" %(data_names[counter], collected_data[counter])\n",
    "    counter += 1\n",
    "display(Markdown(output_table))\n",
    "\n",
    "# Show which fields and filters we're talking about:\n",
    "display(Markdown('Fields: (%i total)' %num_fields))\n",
    "print(fields)\n",
    "display(Markdown('Filters: (%i total)' %num_filters))\n",
    "print(filters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the sky coverage\n",
    "\n",
    "For this we will need our list of `tracts` from above, and also the `skyMap` object. We can then extract the sky coordinates of the corners of each tract, and use them to draw a set of rectangles to illustrate the sky coverage, following Jim Chiang's LSST DESC tutorial [dm_butler_skymap.ipynb](https://github.com/LSSTDESC/DC2-analysis/blob/master/tutorials/dm_butler_skymap.ipynb).\n",
    "\n",
    "In the future, we could imagine overlaying the focal plane and color the individual visits, using more of the code from Jim's notebook. Let's see what functionality the Gen3 Butler provides first, and then return to visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "for tract in tracts:\n",
    "    tractInfo = skyMap[tract]\n",
    "        \n",
    "    corners = [(x[0].asDegrees(), x[1].asDegrees()) for x in tractInfo.getVertexList()]\n",
    "    x = [k[0] for k in corners] + [corners[0][0]]\n",
    "    y = [k[1] for k in corners] + [corners[0][1]]\n",
    "    \n",
    "       \n",
    "    plt.plot(x,y, color='b')\n",
    "    \n",
    "plt.xlabel('RA (deg)')\n",
    "plt.ylabel('Dec (deg)')\n",
    "plt.title('2D Projection of Sky Coverage')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could imagine plotting the patches as well, to show which tracts were incomplete - but this gives us a rough idea of where our data is on the sky."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "We have shown a few techniques for exploring a data repo. To make this process straightforward, we have implemented all these techniques into mehtods of a `Taster` class, which is now a part of the `stackclub` library. The `Taster` will give you a taste of what the `Butler` delivers. we demonstrate the use of this class in the [DataInventory.ipynb](https://github.com/LSSTScienceCollaborations/StackClub/blob/project/data_inventory/drphilmarshall/Basics/DataInventory.ipynb) notebook.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSST",
   "language": "python",
   "name": "lsst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
