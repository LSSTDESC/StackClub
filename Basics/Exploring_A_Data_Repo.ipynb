{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring a Data Repository\n",
    "\n",
    "<br>Owner: **Rob Morgan** ([@rmorgan10](https://github.com/LSSTScienceCollaborations/StackClub/issues/new?body=@rmorgan10)), **Phil Marshall** ([@drphilmarshall](https://github.com/LSSTScienceCollaborations/StackClub/issues/new?body=@drphilmarshall))\n",
    "<br>Last Verified to Run: **2018-12-07**\n",
    "<br>Verified Stack Release: **17.0**\n",
    "\n",
    "This notebook shows how to find out what's in a data repository, and how to find out which inputs went into each component of it.  \n",
    "\n",
    "### Learning Objectives:\n",
    "After working through and studying this notebook you should be able to understand how to use the Butler to figure out: \n",
    "   1. What a data repo is;\n",
    "   2. Which data types are present in a data repository;\n",
    "   3. If coadds have been made, what the available tracts are;\n",
    "   4. Which parts of the sky those tracts cover.\n",
    "   \n",
    "### Logistics\n",
    "This notebook is intended to be runnable on `lsst-lsp-stable.ncsa.illinois.edu` from a local git clone of https://github.com/LSSTScienceCollaborations/StackClub.\n",
    "\n",
    "\n",
    "## Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Markdown\n",
    "import numpy as np\n",
    "import os, glob\n",
    "%matplotlib inline\n",
    "\n",
    "# Filter some warnings printed by v16.0 of the stack\n",
    "# warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "# warnings.simplefilter(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Data Repo?\n",
    "\n",
    "Data repositories contain either a `_mapper` file or a `repositoryCfg.yaml` file, to record which \"obs package\" was used to organize the data. These files give a repository more strucutre and organization than an ordinary data directory. Let's take a look at this file structure in the HSC data repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The HSC Data Repo: What's in there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = '/datasets/hsc/repo'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the `hsc` data repository as our testing ground, and start by figuring out what it contains. In the `hsc` case, the `_mapper` file is in the top level folder, while the data repo for each field is a few levels down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10NP2P31       DEEPE04\t DTH_A\t     OBJECT\t\t     SSP_DEEP_COSMOS\n",
      "11NM5P21       DEEPE05\t FOCUS\t     ON_AXIS_DOMEFLAT\t     SSP_DEEP_DEEP2_3\n",
      "ABELL2163      DEEPE06\t FOCUSING    ON_AXIS_DOMEFLAT_0DEG   SSP_DEEP_ELAIS_N1\n",
      "algorithmData  DEEPE07\t GD153\t     ON_AXIS_DOMEFLAT_72DEG  SSP_DEEP_XMM_LSS\n",
      "BIAS\t       DEEPE08\t HEL10_B     ON_AXIS_DOMEFLAT_90DEG  SSP_DEEP_XMMS_LSS\n",
      "CALIB\t       DEEPE09\t HEL30_I     R10\t\t     SSP_UDEEP_COSMOS\n",
      "CFHTLS_W1      DEEPE10\t HIP67394    ref_cats\t\t     SSP_UDEEP_SXDS\n",
      "COMET2014F3    DEN_A\t HIP67879    registry.sqlite3\t     SSP_WIDE\n",
      "COSMOS\t       DEN_C\t M31\t     rerun\t\t     STRIPE82L\n",
      "DARK\t       DEN_E\t M31_N\t     SDSS_CAL\t\t     SXDS4HSCI2\n",
      "deepCoadd      DITH_14H  M31_S\t     SKYFLAT\t\t     TEST\n",
      "DEEPE01        DITH_16H  _mapper     SP01\t\t     transmission\n",
      "DEEPE02        DITH_D\t MOON_LIGHT  SR01\n",
      "DEEPE03        DOMEFLAT  NAMERICA    SSP_AEGIS\n"
     ]
    }
   ],
   "source": [
    "! ls /datasets/hsc/repo/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the `_mapper` file here, and at contains one line giving the name of the `Mapper` object for the HSC repo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lsst.obs.hsc.HscMapper\n"
     ]
    }
   ],
   "source": [
    "! cat /datasets/hsc/repo/_mapper\n",
    "\n",
    "# Import the Mapper object once you know its name\n",
    "from lsst.obs.hsc import HscMapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get some more information on this object like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(HscMapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mapper defines a (large) number of different \"dataset types\". Some of these are specific to this particular data repo, others are more general. Even filtering out some intermediate dataset types, we are still left with a long list. But, once we figure out which dataset types we are interested in, we can start querying for information about those datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bfKernel', 'bias', 'bias_camera', 'brightObjectMask', 'brighterFatterGain', 'brighterFatterKernel', 'calexp', 'calexpBackground', 'calexpThumb', 'calexp_bbox', 'calexp_calib', 'calexp_camera', 'calexp_detector', 'calexp_filter', 'calexp_visitInfo', 'calexp_wcs', 'calibrated_exp', 'calibrated_exp_bbox', 'calibrated_exp_calib', 'calibrated_exp_detector', 'calibrated_exp_filter', 'calibrated_exp_visitInfo', 'calibrated_exp_wcs', 'calibrated_src', 'camera', 'ccdExposureId', 'ccdExposureId_bits', 'coaddTempExp', 'coaddTempExp_bbox', 'coaddTempExp_calib', 'coaddTempExp_detector', 'coaddTempExp_filter', 'coaddTempExp_visitInfo', 'coaddTempExp_wcs', 'dark', 'dark_camera', 'dcor', 'dcor_bbox', 'dcor_calib', 'dcor_detector', 'dcor_filter', 'dcor_visitInfo', 'dcor_wcs', 'dcrCoadd', 'dcrCoaddId', 'dcrCoaddId_bits', 'dcrCoadd_bbox', 'dcrCoadd_calexp', 'dcrCoadd_calexp_background', 'dcrCoadd_calexp_bbox', 'dcrCoadd_calexp_calib', 'dcrCoadd_calexp_detector', 'dcrCoadd_calexp_filter', 'dcrCoadd_calexp_visitInfo', 'dcrCoadd_calexp_wcs', 'dcrCoadd_calib', 'dcrCoadd_deblendedFlux', 'dcrCoadd_deblendedModel', 'dcrCoadd_det', 'dcrCoadd_detector', 'dcrCoadd_directWarp', 'dcrCoadd_directWarp_bbox', 'dcrCoadd_directWarp_calib', 'dcrCoadd_directWarp_detector', 'dcrCoadd_directWarp_filter', 'dcrCoadd_directWarp_visitInfo', 'dcrCoadd_directWarp_wcs', 'dcrCoadd_filter', 'dcrCoadd_forced_src', 'dcrCoadd_meas', 'dcrCoadd_measMatch', 'dcrCoadd_measMatchFull', 'dcrCoadd_mergeDet', 'dcrCoadd_nImage', 'dcrCoadd_nImage_bbox', 'dcrCoadd_nImage_calib', 'dcrCoadd_nImage_detector', 'dcrCoadd_nImage_filter', 'dcrCoadd_nImage_visitInfo', 'dcrCoadd_nImage_wcs', 'dcrCoadd_ref', 'dcrCoadd_skyMap', 'dcrCoadd_visitInfo', 'dcrCoadd_wcs', 'dcrDiff_diaSrc', 'dcrDiff_differenceExp', 'dcrDiff_differenceExp_bbox', 'dcrDiff_differenceExp_calib', 'dcrDiff_differenceExp_detector', 'dcrDiff_differenceExp_filter', 'dcrDiff_differenceExp_visitInfo', 'dcrDiff_differenceExp_wcs', 'dcrDiff_kernelSrc', 'dcrDiff_matchedExp', 'dcrDiff_matchedExp_bbox', 'dcrDiff_matchedExp_calib', 'dcrDiff_matchedExp_detector', 'dcrDiff_matchedExp_filter', 'dcrDiff_matchedExp_visitInfo', 'dcrDiff_matchedExp_wcs', 'dcrMergedCoaddId', 'dcrMergedCoaddId_bits', 'deepCoadd', 'deepCoaddId', 'deepCoaddId_bits', 'deepCoaddPsfMatched', 'deepCoaddPsfMatched_bbox', 'deepCoaddPsfMatched_calib', 'deepCoaddPsfMatched_detector', 'deepCoaddPsfMatched_filter', 'deepCoaddPsfMatched_visitInfo', 'deepCoaddPsfMatched_wcs', 'deepCoadd_bbox', 'deepCoadd_calexp', 'deepCoadd_calexp_background', 'deepCoadd_calexp_bbox', 'deepCoadd_calexp_calib', 'deepCoadd_calexp_detector', 'deepCoadd_calexp_filter', 'deepCoadd_calexp_hsc', 'deepCoadd_calexp_visitInfo', 'deepCoadd_calexp_wcs', 'deepCoadd_calib', 'deepCoadd_deblendedFlux', 'deepCoadd_deblendedModel', 'deepCoadd_det', 'deepCoadd_detector', 'deepCoadd_diff', 'deepCoadd_diff_bbox', 'deepCoadd_diff_calib', 'deepCoadd_diff_detector', 'deepCoadd_diff_filter', 'deepCoadd_diff_visitInfo', 'deepCoadd_diff_wcs', 'deepCoadd_diffsrc', 'deepCoadd_directWarp', 'deepCoadd_directWarp_bbox', 'deepCoadd_directWarp_calib', 'deepCoadd_directWarp_detector', 'deepCoadd_directWarp_filter', 'deepCoadd_directWarp_visitInfo', 'deepCoadd_directWarp_wcs', 'deepCoadd_filter', 'deepCoadd_forced_src', 'deepCoadd_meas', 'deepCoadd_measMatch', 'deepCoadd_measMatchFull', 'deepCoadd_mergeDet', 'deepCoadd_nImage', 'deepCoadd_nImage_bbox', 'deepCoadd_nImage_calib', 'deepCoadd_nImage_detector', 'deepCoadd_nImage_filter', 'deepCoadd_nImage_visitInfo', 'deepCoadd_nImage_wcs', 'deepCoadd_obj', 'deepCoadd_psfMatchedWarp', 'deepCoadd_psfMatchedWarp_bbox', 'deepCoadd_psfMatchedWarp_calib', 'deepCoadd_psfMatchedWarp_detector', 'deepCoadd_psfMatchedWarp_filter', 'deepCoadd_psfMatchedWarp_visitInfo', 'deepCoadd_psfMatchedWarp_wcs', 'deepCoadd_qa', 'deepCoadd_qa_tract', 'deepCoadd_ref', 'deepCoadd_sg', 'deepCoadd_sg_features', 'deepCoadd_sg_features_tract', 'deepCoadd_skyMap', 'deepCoadd_tempExp_diff', 'deepCoadd_tempExp_diff_bbox', 'deepCoadd_tempExp_diff_calib', 'deepCoadd_tempExp_diff_detector', 'deepCoadd_tempExp_diff_filter', 'deepCoadd_tempExp_diff_visitInfo', 'deepCoadd_tempExp_diff_wcs', 'deepCoadd_tempExp_diffsrc', 'deepCoadd_visitInfo', 'deepCoadd_wcs', 'deepDiff_diaSrc', 'deepDiff_differenceExp', 'deepDiff_differenceExp_bbox', 'deepDiff_differenceExp_calib', 'deepDiff_differenceExp_detector', 'deepDiff_differenceExp_filter', 'deepDiff_differenceExp_visitInfo', 'deepDiff_differenceExp_wcs', 'deepDiff_kernelSrc', 'deepDiff_matchedExp', 'deepDiff_matchedExp_bbox', 'deepDiff_matchedExp_calib', 'deepDiff_matchedExp_detector', 'deepDiff_matchedExp_filter', 'deepDiff_matchedExp_visitInfo', 'deepDiff_matchedExp_wcs', 'deepMergedCoaddId', 'deepMergedCoaddId_bits', 'defects', 'detj', 'detj_bbox', 'detj_calib', 'detj_detector', 'detj_filter', 'detj_visitInfo', 'detj_wcs', 'donutSrc', 'expIdInfo', 'expMetadata', 'expMetadata_bbox', 'expMetadata_calib', 'expMetadata_detector', 'expMetadata_filter', 'expMetadata_visitInfo', 'expMetadata_wcs', 'fcr', 'fcr_bbox', 'fcr_calib', 'fcr_detector', 'fcr_filter', 'fcr_hsc', 'fcr_hsc_bbox', 'fcr_hsc_calib', 'fcr_hsc_detector', 'fcr_hsc_filter', 'fcr_hsc_visitInfo', 'fcr_hsc_wcs', 'fcr_visitInfo', 'fcr_wcs', 'fgcmAtmosphereParameters', 'fgcmFitParameters', 'fgcmFlaggedStars', 'fgcmLookUpTable', 'fgcmStandardStars', 'fgcmStarIds', 'fgcmStarIndices', 'fgcmStarObservations', 'fgcmVisitCatalog', 'fgcmZeropoints', 'fgcm_photoCalib', 'fitsEllPaGrid', 'fitsEllipticityGrid', 'fitsFwhmGrid', 'fitsPsfModelGrid', 'fitsPsfSrcGrid', 'flat', 'flat_camera', 'flattenedImage', 'flattenedImage_bbox', 'flattenedImage_calib', 'flattenedImage_detector', 'flattenedImage_filter', 'flattenedImage_visitInfo', 'flattenedImage_wcs', 'flattenedThumb', 'focusPlot', 'focusSweepPlot', 'forced_src', 'fringe', 'fringe_camera', 'icExp', 'icExpBackground', 'icExp_bbox', 'icExp_calib', 'icExp_detector', 'icExp_filter', 'icExp_visitInfo', 'icExp_wcs', 'icMatch', 'icSrc', 'jointcal_photoCalib', 'jointcal_wcs', 'linearizer', 'log', 'logDir', 'mosaicCalib', 'mosaicCalib_bbox', 'mosaicCalib_calib', 'mosaicCalib_detector', 'mosaicCalib_filter', 'mosaicCalib_visitInfo', 'mosaicCalib_wcs', 'objectTable', 'objectTable_tract', 'ossImage', 'ossImage_bbox', 'ossImage_calib', 'ossImage_detector', 'ossImage_filter', 'ossImage_visitInfo', 'ossImage_wcs', 'ossThumb', 'packages', 'photoCalib', 'plotBrighterFatterPtc', 'plotCoadd', 'plotColor', 'plotCompareCoadd', 'plotCompareVisit', 'plotEllipseGrid', 'plotEllipseMap', 'plotEllipticityGrid', 'plotEllipticityMap', 'plotFwhmGrid', 'plotMagHist', 'plotPsfModelGrid', 'plotPsfSrcGrid', 'plotSeeingMap', 'plotSeeingRobust', 'plotSeeingRough', 'plotSky', 'plotVisit', 'postISRCCD', 'postISRCCD_bbox', 'postISRCCD_calib', 'postISRCCD_detector', 'postISRCCD_filter', 'postISRCCD_visitInfo', 'postISRCCD_wcs', 'psf', 'qaTableCoadd', 'qaTableColor', 'qaTableVisit', 'raw', 'raw_bbox', 'raw_calib', 'raw_detector', 'raw_filter', 'raw_visitInfo', 'raw_wcs', 'ref_cat', 'revexp', 'revexp_bbox', 'revexp_calib', 'revexp_detector', 'revexp_filter', 'revexp_visitInfo', 'revexp_wcs', 'sky', 'skyCorr', 'sky_camera', 'skypolicy', 'src', 'srcMatch', 'srcMatchFull', 'starGalaxy_classifier', 'starGalaxy_morphOnlyClassifier', 'tableSeeingGrid', 'tableSeeingMap', 'transformed_src', 'transmission_atmosphere', 'transmission_atmosphere_fgcm', 'transmission_filter', 'transmission_optics', 'transmission_sensor', 'verify_job', 'warppsf', 'wcs', 'wcs_bbox', 'wcs_calib', 'wcs_detector', 'wcs_filter', 'wcs_hsc', 'wcs_hsc_bbox', 'wcs_hsc_calib', 'wcs_hsc_detector', 'wcs_hsc_filter', 'wcs_hsc_visitInfo', 'wcs_hsc_wcs', 'wcs_visitInfo', 'wcs_wcs', 'yBackground']\n"
     ]
    }
   ],
   "source": [
    "mapper = HscMapper(root=repo)\n",
    "all_dataset_types = mapper.getDatasetTypes()\n",
    "\n",
    "remove = ['_config', '_filename', '_md', '_sub', '_len', '_schema', '_metadata']\n",
    "\n",
    "shortlist = []\n",
    "for dataset_type in all_dataset_types:\n",
    "    keep = True\n",
    "    for word in remove:\n",
    "        if word in dataset_type:\n",
    "            keep = False\n",
    "    if keep:\n",
    "        shortlist.append(dataset_type)\n",
    "\n",
    "print(shortlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Butler`, directed by the `Mapper` will have access to all the above dataset types. \n",
    "\n",
    "Another important file in the repo parent folder is `registry.sqlite3`. This database contains metadata for the HSC raw images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Great, but where's the actual data, and how was it made?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw visit images are stored by field. In the HSC dataset the fields have names like `COSMOS` and `DEEPE09`. Within those field folders, there is a directory structure that eventually gets down to visit image FITS files whose names and paths contain the date/time and filter for that exposure. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/datasets/hsc/repo/COSMOS/2015-01-18/01113/HSC-Y/HSC-0018476-000.fits\n",
      "/datasets/hsc/repo/COSMOS/2015-01-18/01113/HSC-Y/HSC-0018476-001.fits\n",
      "/datasets/hsc/repo/COSMOS/2015-01-18/01113/HSC-Y/HSC-0018476-002.fits\n",
      "/datasets/hsc/repo/COSMOS/2015-01-18/01113/HSC-Y/HSC-0018476-003.fits\n",
      "/datasets/hsc/repo/COSMOS/2015-01-18/01113/HSC-Y/HSC-0018476-004.fits\n",
      "/datasets/hsc/repo/COSMOS/2015-01-18/01113/HSC-Y/HSC-0018476-005.fits\n",
      "/datasets/hsc/repo/COSMOS/2015-01-18/01113/HSC-Y/HSC-0018476-006.fits\n",
      "/datasets/hsc/repo/COSMOS/2015-01-18/01113/HSC-Y/HSC-0018476-007.fits\n",
      "/datasets/hsc/repo/COSMOS/2015-01-18/01113/HSC-Y/HSC-0018476-008.fits\n",
      "/datasets/hsc/repo/COSMOS/2015-01-18/01113/HSC-Y/HSC-0018476-009.fits\n"
     ]
    }
   ],
   "source": [
    "! \\ls /datasets/hsc/repo/COSMOS/2015-01-18/01113/HSC-Y/HSC-0018476-00?.fits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the pipeline tasks know which raw data to process? This information is captured in the \"configs\". In the HSC repo there is no config folder or files in the top level directory - in fact the only two files are `_mapper` and `registry.sqlite3`. So what's going on? \n",
    "\n",
    "It turns out that the provenance of the stack processing of the HSC raw images is captured in  \"rerun\" folders, one for each time the science pipelines were run on the data.  Let's do some detective work to find out what happened to the HSC data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DM-10404  DM-13666  private  RC\n"
     ]
    }
   ],
   "source": [
    "! ls /datasets/hsc/repo/rerun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, `DM-10404` looks like a run ID. What's in that folder? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEEP  logs  SFM  UDEEP\tWIDE\n"
     ]
    }
   ],
   "source": [
    "! ls /datasets/hsc/repo/rerun/DM-10404"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DEEP`, `WIDE`, and `UDEEP` are the names of the sub-surveys of the HSC survey. We might expect each to contain results from the processing of that sub-survey's images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00814  01004  01062  01172  01315  01327  01414\t\t     repositoryCfg.yaml\n",
      "00817  01052  01111  01174  01316  01378  config\t     schema\n",
      "00823  01055  01116  01179  01317  01382  deepCoadd\n",
      "00991  01057  01122  01232  01318  01405  deepCoadd-results\n",
      "00995  01059  01170  01236  01320  01407  jointcal-results\n"
     ]
    }
   ],
   "source": [
    "! ls /datasets/hsc/repo/rerun/DM-10404/UDEEP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numerically-names folders contain the generated catalog files, organized by sky tract. Note that a `config` folder is present, and also a `repositoryCfg.yaml` file - which means that this folder is itself a `repo`, from the Butler's point of view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!RepositoryCfg_v1\n",
      "_mapper: !!python/name:lsst.obs.hsc.hscMapper.HscMapper ''\n",
      "_mapperArgs: null\n",
      "_parents: [../SFM]\n",
      "_policy: null\n",
      "_root: null\n"
     ]
    }
   ],
   "source": [
    "! cat /datasets/hsc/repo/rerun/DM-10404/WIDE/repositoryCfg.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does it mean that `SFM` is the \"parent\" of this repo? Let's see what _that_ folder contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00814  01052  01116  01183  01241  01297  01320  01407\t    metadata\n",
      "00817  01055  01122  01228  01288  01298  01327  01409\t    repositoryCfg.yaml\n",
      "00823  01057  01170  01230  01289  01299  01374  01411\t    schema\n",
      "00991  01058  01172  01232  01290  01315  01378  01413\n",
      "00995  01059  01174  01234  01291  01316  01382  01414\n",
      "01001  01062  01176  01236  01292  01317  01388  config\n",
      "01004  01111  01179  01238  01296  01318  01405  deepCoadd\n"
     ]
    }
   ],
   "source": [
    "! ls /datasets/hsc/repo/rerun/DM-10404/SFM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SFM` seems to contain _all_ the tracts that have been produced - so is the entire HSC survey. It's `repositoryCfg.yaml` file shows that it's \"parent\" is the top level folder, `/datasets/hsc/repo/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!RepositoryCfg_v1\n",
      "_mapper: !!python/name:lsst.obs.hsc.hscMapper.HscMapper ''\n",
      "_mapperArgs: null\n",
      "_parents: [../../..]\n",
      "_policy: null\n",
      "_root: null\n"
     ]
    }
   ],
   "source": [
    "! cat /datasets/hsc/repo/rerun/DM-10404/SFM/repositoryCfg.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the `DM-10404/UDEEP` repo's `config` folder contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".   coaddDriver.py    mosaic.py\t\t  packages.pickle\n",
      "..  forcedPhotCcd.py  multiBandDriver.py\n"
     ]
    }
   ],
   "source": [
    "! ls -a /datasets/hsc/repo/rerun/DM-10404/UDEEP/config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the configuration files that were used when the science pipelines were run on these data. While we don't know which versions of the software were used, we at least know which tasks were run. \n",
    "\n",
    "Notice that `ProcessCcd.py` is not in this list: re-runs can pick up the processing at any point, and just repeat a subset of the tasks. This is why the `parent` has to be recorded: in this case, information about the outputs of `ProcessCcd.py` should be available in the `parent` folder. \n",
    "\n",
    "> **PROBLEM: in the HSC dataset, there doesn't seem to be a config for the basic `ProcessCcd` image processing whose results must be somewhere in the top level (parent) folder. But where are these source tables? **\n",
    "\n",
    "Here's what a config file looks like (ignoring the many import statements and just looking at a few example lines):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assert type(config)==lsst.meas.base.forcedPhotCcd.ForcedPhotCcdConfig, 'config is of type %s.%s instead of lsst.meas.base.forcedPhotCcd.ForcedPhotCcdConfig' % (type(config).__module__, type(config).__name__)\n",
      "# Run subtask to apply aperture corrections\n",
      "config.doApCorr=True\n",
      "\n",
      "# flux measurement algorithms in getApCorrNameSet() to ignore; if a name is listed that does not appear in getApCorrNameSet() then a warning is logged\n",
      "config.applyApCorr.ignoreList=[]\n",
      "\n",
      "# flux measurement algorithms to be aperture-corrected by reference to another algorithm; this is a mapping alg1:alg2, where 'alg1' is the algorithm being corrected, and 'alg2' is the algorithm supplying the corrections\n",
      "config.applyApCorr.proxies={}\n",
      "\n",
      "# set the general failure flag for a flux when it cannot be aperture-corrected?\n",
      "config.applyApCorr.doFlagApCorrFailures=True\n",
      "\n",
      "# coadd name: typically one of deep or goodSeeing\n",
      "config.coaddName='deep'\n",
      "grep: write error\n",
      "cat: write error: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "! cat /datasets/hsc/repo/rerun/DM-10404/UDEEP/config/forcedPhotCcd.py | grep -v import | head -15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next level to dig into here is the tract folders within one of the repos in this rerun. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forcedPhotCcd_metadata\n",
      "FORCEDSRC-0000318-000.fits\n",
      "FORCEDSRC-0000318-001.fits\n",
      "FORCEDSRC-0000318-002.fits\n",
      "FORCEDSRC-0000318-003.fits\n",
      "FORCEDSRC-0000318-004.fits\n",
      "FORCEDSRC-0000318-005.fits\n",
      "FORCEDSRC-0000318-006.fits\n",
      "FORCEDSRC-0000318-007.fits\n",
      "FORCEDSRC-0000318-008.fits\n"
     ]
    }
   ],
   "source": [
    "! ls /datasets/hsc/repo/rerun/DM-10404/UDEEP/00814/HSC-Y/tract9570 | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those FITS files contain the forced source tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiating the Butler and looking for Dataset Types\n",
    "\n",
    "Now that we have an idea of the structure of the repo itself, let's use the Butler to explore the data within the repo. Here we will demonstrate a few useful `Butler` methods for learning about the data in a repo. Let's choose one of the rerun repos, and investigate its properties. We'll summon two butlers, one that is pointed at the parent repo, and another (an \"under butler\") that is asked to focus on a particular sub-survey in a particular re-run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/datasets/hsc/repo/rerun/DM-10404/UDEEP\n"
     ]
    }
   ],
   "source": [
    "parent_repo = '/datasets/hsc/repo'\n",
    "\n",
    "# Choose a re-run repo:\n",
    "rerun_id = 'DM-10404'\n",
    "depth = 'UDEEP'\n",
    "# Try a different one:\n",
    "# rerun = 'DM-13666'\n",
    "# depth = 'WIDE'\n",
    "\n",
    "repo = parent_repo + '/rerun/' + rerun_id + '/' + depth\n",
    "print(repo)\n",
    "\n",
    "from lsst.daf.persistence import Butler\n",
    "\n",
    "butler = Butler(parent_repo)\n",
    "under_butler = Butler(repo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `butler` purports to be able to check whether a datatype (like the source catalogs) actually exists or not, but it needs a specific dataset ID to check whether that specific part of the dataset exists. Here's what you get when you pass in a null dataset ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "butler.datasetExists('src', dataId={})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, one can try querying the repo metadata and checking for an error. Note that this way of checking for dataset existence is a little faster too.\n",
    "\n",
    "Note that the metadata being queried here is in the `registry.sqlite3` database in the _parent_ repo - and so refers to the _initial_ processing run, not the most recent rerun. We'll need to work carefully around this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src dataset exists.\n"
     ]
    }
   ],
   "source": [
    "datasettype = 'src'\n",
    "\n",
    "try:\n",
    "    datasetkeys = butler.getKeys(datasettype)\n",
    "    onekey = list(datasetkeys.keys())[0]\n",
    "    metadata = butler.queryMetadata(datasettype, [onekey])\n",
    "    print(\"{} dataset exists.\".format(datasettype))\n",
    "except:\n",
    "    print(\"{} dataset doesn't exist.\".format(datasettype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, even though its not at all obvious from the file system where the source catalog FITS files are, the `butler` knows where they are based on the metadata in the registry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining Basic Dataset Properties Using the Butler\n",
    "Now we can start using Butler methods to query the data. For this dataset, we can look at the filters used, number of visits, number of pointings, etc. by examining the Butler's keys and metadata. For these basic properties, we will look at the `calexp` and `src` tables. The contents of these tables are derived from the processing of individual sensors, and exist in the parent folder. (That means that we can use either of our two butlers to query for them.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This would be faster if only one query were issued...\n",
    "visits = butler.queryMetadata('calexp', ['visit'])\n",
    "pointings = butler.queryMetadata('calexp', ['pointing'])\n",
    "ccds = butler.queryMetadata('calexp', ['ccd'])\n",
    "fields = butler.queryMetadata('calexp', ['field'])\n",
    "filters = butler.queryMetadata('calexp', ['filter'])\n",
    "sources = butler.queryMetadata('src', ['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_visits = len(visits)\n",
    "num_pointings = len(pointings)\n",
    "num_ccds = len(ccds)\n",
    "num_fields = len(fields)\n",
    "num_filters = len(filters)\n",
    "\n",
    "num_sources = len(sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a check, let's compare what our two butlers find when asked for the number of sources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The butler says that we have 1154258 sources.\n",
      "The under butler says we have 1154258 sources.\n"
     ]
    }
   ],
   "source": [
    "alt_num_sources = len(under_butler.queryMetadata('src', ['id']))\n",
    "\n",
    "print(\"The butler says that we have {:d} sources.\".format(num_sources)) \n",
    "print(\"The under butler says we have {:d} sources.\".format(alt_num_sources))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we could have done our query with the `under_butler` as well. In practice, it's best to specify a Butler for the rerun repo, because that Butler will also have access to the parent repo.\n",
    "\n",
    "One may also be interested in the total sky area imaged for a particular coadd rerun/depth. We can estimate and visualize this from the coadd tract info that neither our `under_butler` nor our `butler` has access to. To collect all the tracts, we have to get them via the file structure. This operation will hopefully be `Butler`-ized with the Gen3 Butler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11 tracts in repo /datasets/hsc/repo/rerun/DM-10404/UDEEP\n"
     ]
    }
   ],
   "source": [
    "# Collect tract indices from file names\n",
    "tracts = sorted([int(os.path.basename(x)) for x in\n",
    "                 glob.glob(os.path.join(repo, 'deepCoadd-results', 'merged', '*'))])\n",
    "num_tracts = len(tracts)\n",
    "\n",
    "print(\"Found {} tracts in repo {}\".format(num_tracts, repo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick way of extimating the sky area covered is to sum the areas of the inner boxes of all the tracts. For more information on the properties of tracts, you can look at the [Documentation](http://doxygen.lsst.codes/stack/doxygen/x_masterDoxyDoc/classlsst_1_1skymap_1_1tract_info_1_1_tract_info.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a quick note, the file structure only tells us the names of the tracts in the particular rerun/depth to look at. The actual `TractInfo` objects are obtained by selecting the tracts we want from the `deepCoadd_skyMap` dataset in our particular rerun repo. Therefore, we will have to ask the `under_butler` to bring us this dataset for the particular rerun/depth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total area imaged (sq deg):  31.04\n"
     ]
    }
   ],
   "source": [
    "# Calculate area from all tracts\n",
    "skyMap = under_butler.get('deepCoadd_skyMap')\n",
    "total_area = 0.0  #deg^2\n",
    "plotting_vertices = []\n",
    "for test_tract in tracts:\n",
    "    # Get inner vertices for tract\n",
    "    tractInfo = skyMap[test_tract]\n",
    "    vertices = tractInfo._vertexCoordList\n",
    "    plotting_vertices.append(vertices)\n",
    "    \n",
    "    #calculate area of box\n",
    "    av_dec = 0.5 * (vertices[2][1] + vertices[0][1])\n",
    "    av_dec = av_dec.asRadians()\n",
    "    delta_ra_raw = vertices[0][0] - vertices[1][0] \n",
    "    delta_ra = delta_ra_raw.asDegrees() * np.cos(av_dec)\n",
    "    delta_dec= vertices[2][1] - vertices[0][1]\n",
    "    area = delta_ra * delta_dec.asDegrees()\n",
    "    \n",
    "    #combine areas\n",
    "    total_area += area\n",
    "    \n",
    "\n",
    "# Round off the total area for presentation purposes\n",
    "rounded_total_area = round(total_area, 2)\n",
    "\n",
    "print(\"Total area imaged (sq deg): \",rounded_total_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The HSC DM-10404/UDEEP rerun contains 10333 visits.\n",
      "The HSC DM-10404/UDEEP rerun contains 133 pointings.\n",
      "The HSC DM-10404/UDEEP rerun contains 112 ccds.\n",
      "The HSC DM-10404/UDEEP rerun contains 60 fields.\n",
      "The HSC DM-10404/UDEEP rerun contains 13 filters.\n",
      "The HSC DM-10404/UDEEP rerun contains 1154258 sources.\n"
     ]
    }
   ],
   "source": [
    "print(\"The HSC {}/{} rerun contains {} visits.\".format(rerun_id,depth,num_visits))\n",
    "print(\"The HSC {}/{} rerun contains {} pointings.\".format(rerun_id,depth,num_pointings))\n",
    "print(\"The HSC {}/{} rerun contains {} ccds.\".format(rerun_id,depth,num_ccds))\n",
    "print(\"The HSC {}/{} rerun contains {} fields.\".format(rerun_id,depth,num_fields))\n",
    "print(\"The HSC {}/{} rerun contains {} filters.\".format(rerun_id,depth,num_filters))\n",
    "print(\"The HSC {}/{} rerun contains {} sources.\".format(rerun_id,depth,num_sources))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying Dataset Characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's print out a report of all the characteristcs we have found. We'll use the sky area from the rerun we chose, and the numbers common to all reruns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### /datasets/hsc/repo/rerun/DM-10404/UDEEP"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "|   Metadata Characteristic  | Value | \n",
       "  | ---: | ---: | \n",
       " | Number of Visits |  10333 | \n",
       "| Number of Pointings |  133 | \n",
       "| Number of CCDs |  112 | \n",
       "| Number of Fields |  60 | \n",
       "| Number of Filters |  13 | \n",
       "| Number of Sources |  1154258 | \n",
       "| Number of Tracts |  11 | \n",
       "| Total Sky Area (deg$^2$) |  31.04 | \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Fields: (60 total)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DOMEFLAT', 'DEN_E', 'BIAS', 'DEN_C', '10NP2P31', 'HEL30_I', 'MOON_LIGHT', 'COSMOS', 'DARK', 'HEL10_B', 'GD153', 'OBJECT', '11NM5P21', 'DEN_A', 'HIP67394', 'DITH_16H', 'ABELL2163', 'SKYFLAT', 'M31_N', 'M31_S', 'HIP67879', 'M31', 'NAMERICA', 'STRIPE82L', 'DITH_14H', 'CFHTLS_W1', 'DTH_A', 'FOCUS', 'SP01', 'DEEPE09', 'DITH_D', 'FOCUSING', 'DEEPE08', 'TEST', 'DEEPE02', 'DEEPE03', 'DEEPE06', 'DEEPE07', 'DEEPE01', 'COMET2014F3', 'DEEPE05', 'DEEPE10', 'DEEPE04', 'R10', 'SR01', 'SDSS_CAL', 'SSP_WIDE', 'SSP_UDEEP_SXDS', 'SSP_UDEEP_COSMOS', 'ON_AXIS_DOMEFLAT_90DEG', 'SSP_DEEP_DEEP2_3', 'ON_AXIS_DOMEFLAT_0DEG', 'SSP_AEGIS', 'SSP_DEEP_COSMOS', 'SSP_DEEP_ELAIS_N1', 'SSP_DEEP_XMM_LSS', 'SSP_DEEP_XMMS_LSS', 'ON_AXIS_DOMEFLAT', 'ON_AXIS_DOMEFLAT_72DEG', 'SXDS4HSCI2']\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Filters: (13 total)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HSC-Z', 'HSC-G', 'ENG-R1', '109', 'HSC-I', 'NB0921', 'HSC-R', 'SH', 'HSC-Y', 'NB0816', 'NB0656', 'HSC-I2', 'NB0515']\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'HSC'\n",
    "display(Markdown('### %s' % repo))\n",
    "\n",
    "\n",
    "# Make a table of the collected metadata\n",
    "collected_data = [num_visits, num_pointings, num_ccds, num_fields, num_filters, num_sources, \n",
    "                  num_tracts, rounded_total_area]\n",
    "data_names = (\"Number of Visits\", \"Number of Pointings\", \"Number of CCDs\", \"Number of Fields\", \n",
    "              \"Number of Filters\", \"Number of Sources\", \"Number of Tracts\", \"Total Sky Area (deg$^2$)\")\n",
    "\n",
    "output_table = \"|   Metadata Characteristic  | Value | \\n  | ---: | ---: | \\n \"\n",
    "counter = 0\n",
    "while counter < len(collected_data):\n",
    "    output_table += \"| %s |  %s | \\n\" %(data_names[counter], collected_data[counter])\n",
    "    counter += 1\n",
    "display(Markdown(output_table))\n",
    "\n",
    "# Show which fields and filters we're talking about:\n",
    "display(Markdown('Fields: (%i total)' %num_fields))\n",
    "print(fields)\n",
    "display(Markdown('Filters: (%i total)' %num_filters))\n",
    "print(filters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the sky coverage\n",
    "\n",
    "For this we will need our list of `tracts` from above, and also the `skyMap` object. We can then extract the sky coordinates of the corners of each tract, and use them to draw a set of rectangles to illustrate the sky coverage, following Jim Chiang's LSST DESC tutorial [dm_butler_skymap.ipynb](https://github.com/LSSTDESC/DC2-analysis/blob/master/tutorials/dm_butler_skymap.ipynb).\n",
    "\n",
    "In the future, we could imagine overlaying the focal plane and color the individual visits, using more of the code from Jim's notebook. Let's see what functionality the Gen3 Butler provides first, and then return to visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "\n",
    "for tract in tracts:\n",
    "    tractInfo = skyMap[tract]\n",
    "        \n",
    "    corners = [(x[0].asDegrees(), x[1].asDegrees()) for x in tractInfo.getVertexList()]\n",
    "    x = [k[0] for k in corners] + [corners[0][0]]\n",
    "    y = [k[1] for k in corners] + [corners[0][1]]\n",
    "    \n",
    "       \n",
    "    plt.plot(x,y, color='b')\n",
    "    \n",
    "plt.xlabel('RA (deg)')\n",
    "plt.ylabel('Dec (deg)')\n",
    "plt.title('2D Projection of Sky Coverage')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could imagine plotting the patches as well, to show which tracts were incomplete - but this gives us a rough idea of where our data is on the sky."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "We have shown a few techniques for exploring a data repo. To make this process straightforward, we have implemented all these techniques into mehtods of a `Taster` class, which is now a part of the `stackclub` library. The `Taster` will give you a taste of what the `Butler` delivers. we demonstrate the use of this class in the [DataInventory.ipynb](https://github.com/LSSTScienceCollaborations/StackClub/blob/project/data_inventory/drphilmarshall/Basics/DataInventory.ipynb) notebook.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSST",
   "language": "python",
   "name": "lsst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
