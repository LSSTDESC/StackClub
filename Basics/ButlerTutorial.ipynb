{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Access with the Gen-2 Butler\n",
    "\n",
    "<br>Owner: **Daniel Perrefort** ([@djperrefort](https://github.com/LSSTScienceCollaborations/StackClub/issues/new?body=@djperrefort))\n",
    "<br>Last Verified to Run: **2019-08-08**\n",
    "<br>Verified Stack Release: **v18.1.0**\n",
    "\n",
    "## Core Concepts\n",
    "\n",
    "This notebook provides a hands on overview of how to interact with the Gen-2 `Butler` (it should be updated for Gen-3, once available). The `Butler` provides a way to access information using a uniform interface without needing to keep track of how the information is internally stored or organized. Data access with `Butler` has three levels you need to be aware of:\n",
    "\n",
    "1. Each instantiated `Butler` object provides access to a collection of datasets called a **repository**. Each repository is defined by Butler using the local file directory where the data is stored.\n",
    "1. Each data set in a **repository** is assigned a unique name called a **type**. These types are strings that describe the data set and should not be confused with an \"object type\" as defined by Python.\n",
    "1. Individual entries in a data set are identified using a unique **data identifier**, which is a dictionary who's allowed keys and values depend on the data set you are working with.\n",
    "\n",
    "## Learning Objectives:\n",
    "\n",
    "This notebook demonstrates how to use the Gen-2 `Butler` object from the DM stack to parse and manipulate data. After finishing this notebook, users will know how to:\n",
    "\n",
    "- Load and access a data repository using `Butler`\n",
    "- Select subsets of data and convert data into familiar data structures\n",
    "- Use `Butler` to access coordinate information and cutout postage stamps\n",
    "- Use `Butler` to access a skymap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import lsst.afw.display as afwDisplay\n",
    "import lsst.afw.geom as afwGeom\n",
    "import matplotlib.pyplot as plt\n",
    "from lsst.daf.persistence import Butler\n",
    "from lsst.geom import SpherePoint\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "To start we instantialize a `Butler` object by providing it with the directory of the **repository** we want to access. Next we load a **type** of dataset and select data from a single **data identifier**. For this demonstration we consider the `deepCoadd_ref` dataset which contains tables of information concerning coadded images used in the differencing image pipeline. The id values for this data set include two required values: `tract` and `patch` which denote sections of the sky.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#repo = '/project/shared/data/diffim_template'\n",
    "#repo = '/project/shared/data/DATA_ci_hsc/rerun/coadd'\n",
    "repo = '/project/shared/data/DATA_ci_hsc/rerun/coaddForcedPhot'\n",
    "butler = Butler(repo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "butler.get('deepCoadd_ref', {'tract': 0, 'patch': '1,1'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We choose an arbitrary tract and patch. \n",
    "data_id = {'tract': 0, 'patch': '1,1'}\n",
    "dataset_type = 'deepCoadd_ref'\n",
    "\n",
    "# We can check that the data exists before we try to read it\n",
    "data_exists = butler.datasetExists(datasetType=dataset_type, dataId=data_id)\n",
    "print('Data exists for ID:', data_exists)\n",
    "\n",
    "data_entry = butler.get(dataset_type, dataId=data_id)\n",
    "data_entry\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data table returned above is formatted as a `SourceCatalog` object, which is essentially a collection of `numpy` arrays. We can see this when we index a particular column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(data_entry['merge_measurement_i']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SourceCatalog` objects have their own set of methods for table manipulations (sorting, appending rows, et.). However, we can also work with the data in a more familiar format, such as an astropy `Table` or a pandas `DataFrame`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = data_entry.asAstropy().to_pandas()\n",
    "data_frame.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that `Butler` objects are not required to return tabular data. We will see an example of this later when we load and parse image data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Subsets of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice you may not know the format of the data identifier for a given data set. In this case the `getKeys()` method can be used to determine the key values expected in a **data identifier**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_id_format = butler.getKeys(dataset_type)\n",
    "print('Expected data id format:', data_id_format)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is import to note that the `getKeys` method will only specify key values that are required by the data set. We can also include optional key values to further select a subset of the data. Lets say we only wanted table entries where `merge_footprint_i` has a value of `True`. We could retrieve the data and subselect the values of the returned table, or we can do simply specify the condition in the data id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't do this!\n",
    "#new_data_id = {'tract': 0, 'patch': '1,1', 'merge_footprint_i': True}\n",
    "# Do this...\n",
    "new_data_id = {'tract': 0, 'patch': '1,1'}\n",
    "merged_i_data = butler.get(dataset_type, dataId=new_data_id)\n",
    "\n",
    "# Check that the returned table does in fact have only entries where\n",
    "# merge_footprint_i is True\n",
    "# Need to copy or else non-contiguous in memory (yay)\n",
    "merged_i_data = merged_i_data[merged_i_data['merge_measurement_i']].copy(True)\n",
    "\n",
    "print(merged_i_data['merge_measurement_i'].all())\n",
    "merged_i_data.asAstropy().to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where( (merged_i_data['base_PsfFlux_flux'] > 100) & (merged_i_data['base_PsfFlux_flux'] < 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "i = 1000\n",
    "ra =  np.degrees(merged_i_data['coord_ra'][i])\n",
    "dec = np.degrees(merged_i_data['coord_dec'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although you can't specify complex conditionals in a data id, you can use it so perform simple selections. You can also can also select a select all complete dataIds for a dataset type that match a partial (or empty) dataId. For example, the below cell iterates over all possible ids and checks if the corresponding file exists.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = butler.subset(dataset_type, dataId=data_id)\n",
    "id_list = [dr.dataId for dr in subset if dr.datasetExists()]\n",
    "print(f'fAvailable Ids:\\n {id_list}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Postage Stamps\n",
    "\n",
    "When dealing with image data, we can use `Butler` to generate postage stamps at a given set of coordinates. For this example we consider the `deepCoadd` data set, which has one extra key value than the previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coadd_type = 'deepCoadd'\n",
    "butler.getKeys(coadd_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to generate a postage stamp, we define the center an size of the cutout using "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the image using butler\n",
    "coadd_id = {'tract': 0, 'patch': '1,1', 'filter': 'HSC-I'}\n",
    "image = butler.get(coadd_type, dataId=coadd_id)\n",
    "\n",
    "# Define the center and size of our cutout\n",
    "#ra, dec = 53.135801, -28.426165\n",
    "#ra, dec = 320.97720068290386, -0.4069160376552347\n",
    "radec = SpherePoint(ra, dec, afwGeom.degrees)\n",
    "cutout_size = 150\n",
    "cutout_extent = afwGeom.ExtentI(cutout_size, cutout_size)\n",
    "\n",
    "# Cutout and optionally save the postage stamp to file\n",
    "postage_stamp = image.getCutout(radec, cutout_extent)\n",
    "# postage_stamp.writeFits(<output_filename>)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the postage stamp was generated using `Butler`, we can plot it using the DM `afwDisplay` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = postage_stamp.getWcs().skyToPixel(radec)\n",
    "\n",
    "display = afwDisplay.Display(frame=1, backend='matplotlib')\n",
    "display.mtv(postage_stamp)\n",
    "display.scale(\"linear\", \"zscale\")\n",
    "display.dot('o', xy.getX(), xy.getY(), ctype='red')\n",
    "display.show_colorbar()\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that the cutout image is aware of what the pixel values were in the original image. This is why the axis labels in the above cutout are so large. We also note that the orientation of the postage stamp is in the x, y orientation of the original coadded image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting an Area on the Sky with a Sky Map\n",
    "\n",
    "As a final example, we consider a third type of data that can be accessed via `Butler` called `skymap`. Sky maps allow you to look up information for a given `tract` and `patch`. You may notice from the below example that data set **types** tend to follow the convertion of having a base name (e.g. `'deepCoadd'`) followed by a descriptor (e.g. `'_skyMap'`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skymap = butler.get('deepCoadd_skyMap')\n",
    "tract_info = skymap[0]\n",
    "tract_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_info = tract_info.getPatchInfo((1,1))\n",
    "patch_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tract_bbox = tract_info.getBBox()\n",
    "tract_pix_corners = afwGeom.Box2D(tract_bbox).getCorners()\n",
    "print('Tract corners in pixels:\\n', tract_pix_corners)\n",
    "\n",
    "wcs = tract_info.getWcs()\n",
    "tract_deg_corners = wcs.pixelToSky(tract_pix_corners)\n",
    "tract_deg_corners = [[c.getRa().asDegrees(), c.getDec().asDegrees()] for c in tract_deg_corners]\n",
    "print('\\nTract corners in degrees:\\n', tract_deg_corners)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSST",
   "language": "python",
   "name": "lsst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
